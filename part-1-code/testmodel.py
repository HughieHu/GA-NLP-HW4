# åœ¨ import éƒ¨åˆ†ä¹‹åï¼Œå…¶ä»–å‡½æ•°ä¹‹å‰æ·»åŠ è¿™ä¸ªè¯Šæ–­å‡½æ•°
import datasets
from datasets import load_dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification
from torch.optim import AdamW
from transformers import get_scheduler
import torch
from tqdm.auto import tqdm
import evaluate
import random
import argparse
from utils import *
import os

# Set seed
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def diagnose_model(model, eval_dataloader, device, num_batches=5):
    """
    è¯Šæ–­æ¨¡å‹è¡Œä¸ºï¼Œåˆ¤æ–­æ˜¯éšæœºçŒœæµ‹è¿˜æ˜¯ç³»ç»Ÿæ€§é”™è¯¯
    """
    print("\n" + "=" * 80)
    print("ğŸ”¬ æ¨¡å‹è¯Šæ–­å¼€å§‹")
    print("=" * 80)
    
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_logits = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(eval_dataloader):
            if batch_idx >= num_batches:
                break
                
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(predictions.cpu().numpy().tolist())
            all_labels.extend(batch["labels"].cpu().numpy().tolist())
            all_logits.extend(logits.cpu().numpy().tolist())
    
    # ç»Ÿè®¡åˆ†æ
    from collections import Counter
    pred_counter = Counter(all_predictions)
    label_counter = Counter(all_labels)
    
    print(f"\nğŸ“Š é¢„æµ‹åˆ†å¸ƒï¼ˆå‰ {num_batches} ä¸ªæ‰¹æ¬¡ï¼‰:")
    print(f"   æ ‡ç­¾ 0 (è´Ÿé¢): {pred_counter[0]} æ¬¡")
    print(f"   æ ‡ç­¾ 1 (æ­£é¢): {pred_counter[1]} æ¬¡")
    
    print(f"\nğŸ“Š çœŸå®æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"   æ ‡ç­¾ 0 (è´Ÿé¢): {label_counter[0]} ä¸ª")
    print(f"   æ ‡ç­¾ 1 (æ­£é¢): {label_counter[1]} ä¸ª")
    
    # æ£€æŸ¥æ˜¯å¦æ€»æ˜¯é¢„æµ‹åŒä¸€ä¸ªç±»åˆ«
    if len(pred_counter) == 1:
        only_pred = list(pred_counter.keys())[0]
        print(f"\nâš ï¸  è­¦å‘Šï¼šæ¨¡å‹æ€»æ˜¯é¢„æµ‹ç±»åˆ« {only_pred}ï¼")
        print("   è¿™è¡¨æ˜æ¨¡å‹å·²ç»å´©æºƒæˆ–æœªæ­£ç¡®è®­ç»ƒã€‚")
        return "collapsed"
    
    # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
    correct = sum(1 for p, l in zip(all_predictions, all_labels) if p == l)
    accuracy = correct / len(all_predictions)
    
    print(f"\nâœ… å±€éƒ¨å‡†ç¡®ç‡: {accuracy:.4f} ({correct}/{len(all_predictions)})")
    
    # æ£€æŸ¥ logits åˆ†å¸ƒ
    import numpy as np
    logits_array = np.array(all_logits)
    logits_0 = logits_array[:, 0]
    logits_1 = logits_array[:, 1]
    
    print(f"\nğŸ“ˆ Logits ç»Ÿè®¡:")
    print(f"   ç±»åˆ« 0 logits - å‡å€¼: {np.mean(logits_0):.3f}, æ ‡å‡†å·®: {np.std(logits_0):.3f}")
    print(f"   ç±»åˆ« 1 logits - å‡å€¼: {np.mean(logits_1):.3f}, æ ‡å‡†å·®: {np.std(logits_1):.3f}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡ç­¾åè½¬
    reversed_correct = sum(1 for p, l in zip(all_predictions, all_labels) if p == (1 - l))
    reversed_accuracy = reversed_correct / len(all_predictions)
    
    if reversed_accuracy > 0.7:
        print(f"\nâš ï¸  æ ‡ç­¾å¯èƒ½åè½¬ï¼å¦‚æœåè½¬æ ‡ç­¾ï¼Œå‡†ç¡®ç‡ä¸º: {reversed_accuracy:.4f}")
        return "reversed_labels"
    
    # æ£€æŸ¥æ˜¯å¦éšæœºçŒœæµ‹
    if 0.45 <= accuracy <= 0.55:
        # è¿›ä¸€æ­¥æ£€æŸ¥ logits çš„å·®å¼‚
        logit_diffs = logits_1 - logits_0
        avg_diff = np.mean(np.abs(logit_diffs))
        
        print(f"\nğŸ² Logits å·®å¼‚å‡å€¼: {avg_diff:.4f}")
        
        if avg_diff < 0.5:
            print("   â†’ å·®å¼‚å¾ˆå°ï¼Œæ¨¡å‹å¯èƒ½æ¥è¿‘éšæœºçŒœæµ‹")
            return "random"
        else:
            print("   â†’ Logits æœ‰æ˜æ˜¾å·®å¼‚ï¼Œä½†å‡†ç¡®ç‡ä½å¯èƒ½æ˜¯å…¶ä»–åŸå› ")
            return "other_issue"
    
    # æ˜¾ç¤ºä¸€äº›å…·ä½“æ ·æœ¬
    print(f"\nğŸ“‹ å‰ 10 ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯:")
    print(f"{'ç´¢å¼•':<6} {'çœŸå®':<6} {'é¢„æµ‹':<6} {'Logit[0]':<10} {'Logit[1]':<10} {'ç»“æœ':<6}")
    print("-" * 60)
    
    for i in range(min(10, len(all_predictions))):
        result = "âœ…" if all_predictions[i] == all_labels[i] else "âŒ"
        print(f"{i:<6} {all_labels[i]:<6} {all_predictions[i]:<6} "
              f"{all_logits[i][0]:<10.3f} {all_logits[i][1]:<10.3f} {result:<6}")
    
    print("=" * 80)
    
    return "normal"


def test_model_with_examples(model_dir, device):
    """
    ç”¨æ˜ç¡®çš„æµ‹è¯•æ ·æœ¬æµ‹è¯•æ¨¡å‹
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª æ‰‹åŠ¨æ ·æœ¬æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    model.to(device)
    model.eval()
    
    # æµ‹è¯•æ ·æœ¬ - éå¸¸æ˜ç¡®çš„æƒ…æ„Ÿ
    test_samples = [
        ("This movie is absolutely wonderful and amazing!", 1),
        ("Terrible film, complete waste of time and money.", 0),
        ("Best movie ever! Loved every second of it!", 1),
        ("Boring, dull, and utterly disappointing.", 0),
        ("Fantastic performances and brilliant direction!", 1),
        ("Worst movie I have ever seen in my life.", 0),
        ("Incredible story, amazing acting, perfect!", 1),
        ("Awful, horrible, do not watch this garbage.", 0),
    ]
    
    correct = 0
    results = []
    
    for text, true_label in test_samples:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            pred_label = torch.argmax(logits, dim=-1).item()
            confidence = torch.softmax(logits, dim=-1).max().item()
        
        is_correct = pred_label == true_label
        if is_correct:
            correct += 1
        
        results.append({
            'text': text,
            'true': true_label,
            'pred': pred_label,
            'correct': is_correct,
            'logits': logits.cpu().numpy()[0],
            'confidence': confidence
        })
    
    # æ‰“å°ç»“æœ
    print(f"\n{'ç»“æœ':<4} {'çœŸå®':<6} {'é¢„æµ‹':<6} {'ç½®ä¿¡åº¦':<10} æ–‡æœ¬")
    print("-" * 80)
    
    for r in results:
        status = "âœ…" if r['correct'] else "âŒ"
        print(f"{status:<4} {r['true']:<6} {r['pred']:<6} {r['confidence']:<10.3f} {r['text'][:50]}")
    
    accuracy = correct / len(test_samples)
    print(f"\næ€»ä½“å‡†ç¡®ç‡: {correct}/{len(test_samples)} = {accuracy:.2%}")
    
    # åˆ†æç»“æœ
    if accuracy == 0.0:
        print("\nâš ï¸  å®Œå…¨é”™è¯¯ï¼æ ‡ç­¾å¯èƒ½å®Œå…¨åè½¬ï¼")
    elif accuracy == 1.0:
        print("\nâœ… å®Œç¾ï¼æ¨¡å‹å·¥ä½œæ­£å¸¸ï¼")
    elif 0.45 <= accuracy <= 0.55:
        print("\nğŸ² æ¥è¿‘éšæœºçŒœæµ‹ï¼å¯èƒ½çš„åŸå› ï¼š")
        print("   1. æ¨¡å‹æœªè®­ç»ƒæˆ–è®­ç»ƒå¤±è´¥")
        print("   2. åŠ è½½äº†é”™è¯¯çš„æ£€æŸ¥ç‚¹")
        print("   3. æ•°æ®é¢„å¤„ç†æœ‰é—®é¢˜")
    else:
        print(f"\nğŸ“Š å‡†ç¡®ç‡ {accuracy:.2%} - æ¨¡å‹æœ‰ä¸€å®šåŒºåˆ†èƒ½åŠ›ä½†ä¸ç†æƒ³")
    
    # æ£€æŸ¥ logits æ¨¡å¼
    print(f"\nğŸ“ˆ Logits åˆ†æ:")
    for i, r in enumerate(results[:5]):
        print(f"   æ ·æœ¬ {i+1}: [è´Ÿé¢: {r['logits'][0]:.3f}, æ­£é¢: {r['logits'][1]:.3f}]")
    
    print("=" * 80)
    
    return accuracy
    

# Tokenize the input
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


# Core training function
def do_train(args, model, train_dataloader, save_dir="./out"):
    optimizer = AdamW(model.parameters(), lr=args.learning_rate)
    num_epochs = args.num_epochs
    num_training_steps = num_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
    )
    model.train()
    progress_bar = tqdm(range(num_training_steps))

    ################################
    ##### YOUR CODE BEGINGS HERE ###

    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)
    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)
    # You can use progress_bar.update(1) to see the progress during training
    # You can refer to the pytorch tutorial covered in class for reference

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(**batch)
            loss = outputs.loss

            loss.backward()

            optimizer.step()
            lr_scheduler.step()

            optimizer.zero_grad()

            progress_bar.update(1)
            progress_bar.set_postfix({"loss": loss.item()})

    ##### YOUR CODE ENDS HERE ######

    print("Training completed...")
    print("Saving Model....")
    model.save_pretrained(save_dir)

    return


# Core evaluation function
def do_eval(eval_dataloader, output_dir, out_file):
    model = AutoModelForSequenceClassification.from_pretrained(output_dir)
    model.to(device)
    model.eval()

    # ========== æ·»åŠ è¯Šæ–­ ==========
    print("\nğŸ” å¼€å§‹è¯Šæ–­æ¨¡å‹...")
    diagnosis_result = diagnose_model(model, eval_dataloader, device, num_batches=10)
    print(f"è¯Šæ–­ç»“æœ: {diagnosis_result}")
    
    # æ‰‹åŠ¨æ ·æœ¬æµ‹è¯•
    test_accuracy = test_model_with_examples(output_dir, device)
    # ========== è¯Šæ–­ç»“æŸ ==========

    metric = evaluate.load("accuracy")
    out_file = open(out_file, "w")

    for batch in tqdm(eval_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])

        # write to output file
        for pred, label in zip(predictions, batch["labels"]):
                out_file.write(f"{pred.item()}\n")
                out_file.write(f"{label.item()}\n")
    
    out_file.close()
    score = metric.compute()
    
    # ========== æœ€ç»ˆåˆ†æ ==========
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ")
    print("=" * 80)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {score['accuracy']:.4f}")
    print(f"æ‰‹åŠ¨æ ·æœ¬å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"è¯Šæ–­ç»“æœ: {diagnosis_result}")
    
    if score['accuracy'] < 0.55 and test_accuracy < 0.55:
        print("\nâš ï¸  ç»“è®ºï¼šæ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘éšæœºçŒœæµ‹")
        print("\nå¯èƒ½çš„åŸå› :")
        print("   1. æ¨¡å‹æœªæ­£ç¡®è®­ç»ƒï¼ˆæ£€æŸ¥è®­ç»ƒlossï¼‰")
        print("   2. åŠ è½½äº†æœªè®­ç»ƒçš„æ¨¡å‹")
        print("   3. æ ‡ç­¾æ˜ å°„é”™è¯¯")
        print("   4. æ•°æ®é¢„å¤„ç†æœ‰é—®é¢˜")
    elif score['accuracy'] < 0.2:
        print("\nâš ï¸  ç»“è®ºï¼šæ ‡ç­¾å¯èƒ½å®Œå…¨åè½¬")
    
    print("=" * 80)
    # ========== åˆ†æç»“æŸ ==========

    return score


# Created a dataladoer for the augmented training dataset
def create_augmented_dataloader(args, dataset):
    ################################
    ##### YOUR CODE BEGINGS HERE ###

    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' -- this
    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.
    # You may find it helpful to see how the dataloader was created at other place in this code.

    original_train_dataset = dataset["train"]

    augmented_dataset = original_train_dataset.shuffle(seed=42).select(range(5000))

    augmented_dataset = augmented_dataset.map(custom_transform, load_from_cache_file=False)

    from datasets import concatenate_datasets
    combined_dataset = concatenate_datasets([original_train_dataset, augmented_dataset])

    combined_tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)

    combined_tokenized_dataset = combined_tokenized_dataset.remove_columns(["text"])
    combined_tokenized_dataset = combined_tokenized_dataset.rename_column("label", "labels")
    combined_tokenized_dataset.set_format("torch")

    if args.debug_train:
        small_combined_dataset = combined_tokenized_dataset.shuffle(seed=42).select(range(4000))
        train_dataloader = DataLoader(small_combined_dataset, shuffle=True, batch_size=args.batch_size)
    else:
        train_dataloader = DataLoader(combined_tokenized_dataset, shuffle=True, batch_size=args.batch_size)
    
    print(f"Original training dataset size: {len(original_train_dataset)}")
    print(f"Augmented samples: 5000")
    print(f"Combined dataset size: {len(combined_tokenized_dataset)}")
    print(f"len(train_dataloader): {len(train_dataloader)}")

    ##### YOUR CODE ENDS HERE ######

    return train_dataloader


# Create a dataloader for the transformed test set
def create_transformed_dataloader(args, dataset, debug_transformation):
    # Print 5 random transformed examples
    if debug_transformation:
        small_dataset = dataset["test"].shuffle(seed=42).select(range(5))
        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)
        for k in range(5):
            print("Original Example ", str(k))
            print(small_dataset[k])
            print("\n")
            print("Transformed Example ", str(k))
            print(small_transformed_dataset[k])
            print('=' * 30)

        exit()

    transformed_dataset = dataset["test"].map(custom_transform, load_from_cache_file=False)
    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)
    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns(["text"])
    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column("label", "labels")
    transformed_tokenized_dataset.set_format("torch")

    transformed_val_dataset = transformed_tokenized_dataset
    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=args.batch_size)

    return eval_dataloader


if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    # Arguments
    parser.add_argument("--train", action="store_true", help="train a model on the training data")
    parser.add_argument("--train_augmented", action="store_true", help="train a model on the augmented training data")
    parser.add_argument("--eval", action="store_true", help="evaluate model on the test set")
    parser.add_argument("--eval_transformed", action="store_true", help="evaluate model on the transformed test set")
    parser.add_argument("--model_dir", type=str, default="./out")
    parser.add_argument("--debug_train", action="store_true",
                        help="use a subset for training to debug your training loop")
    parser.add_argument("--debug_transformation", action="store_true",
                        help="print a few transformed examples for debugging")
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=8)

    args = parser.parse_args()

    global device
    global tokenizer

    # Device
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    # Tokenize the dataset
    dataset = load_dataset("imdb")
    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # Prepare dataset for use by model
    tokenized_dataset = tokenized_dataset.remove_columns(["text"])
    tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
    tokenized_dataset.set_format("torch")

    small_train_dataset = tokenized_dataset["train"].shuffle(seed=42).select(range(4000))
    small_eval_dataset = tokenized_dataset["test"].shuffle(seed=42).select(range(1000))

    # Create dataloaders for iterating over the dataset
    if args.debug_train:
        train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=args.batch_size)
        eval_dataloader = DataLoader(small_eval_dataset, batch_size=args.batch_size)
        print(f"Debug training...")
        print(f"len(train_dataloader): {len(train_dataloader)}")
        print(f"len(eval_dataloader): {len(eval_dataloader)}")
    else:
        train_dataloader = DataLoader(tokenized_dataset["train"], shuffle=True, batch_size=args.batch_size)
        eval_dataloader = DataLoader(tokenized_dataset["test"], batch_size=args.batch_size)
        print(f"Actual training...")
        print(f"len(train_dataloader): {len(train_dataloader)}")
        print(f"len(eval_dataloader): {len(eval_dataloader)}")

    # Train model on the original training dataset
    if args.train:
        model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
        model.to(device)
        do_train(args, model, train_dataloader, save_dir="./out")
        # Change eval dir
        args.model_dir = "./out"

    # Train model on the augmented training dataset
    if args.train_augmented:
        train_dataloader = create_augmented_dataloader(args, dataset)
        model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
        model.to(device)
        do_train(args, model, train_dataloader, save_dir="./out_augmented")
        # Change eval dir
        args.model_dir = "./out_augmented"

    # Evaluate the trained model on the original test dataset
    if args.eval:
        out_file = os.path.basename(os.path.normpath(args.model_dir))
        out_file = out_file + "_original.txt"
        score = do_eval(eval_dataloader, args.model_dir, out_file)
        print("Score: ", score)

    # Evaluate the trained model on the transformed test dataset
    if args.eval_transformed:
        out_file = os.path.basename(os.path.normpath(args.model_dir))
        out_file = out_file + "_transformed.txt"
        eval_transformed_dataloader = create_transformed_dataloader(args, dataset, args.debug_transformation)
        score = do_eval(eval_transformed_dataloader, args.model_dir, out_file)
        print("Score: ", score)

